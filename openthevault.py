# -*- coding: utf-8 -*-
"""OpenTheVault.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ydT7cvNn0FhnAj8ZhPojToOBsiC5Djom

> # We are using Ollama so we can run the code on CPU too

# Installing Required Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !curl -fsSL https://ollama.com/install.sh | sh
# !sudo apt install -y neofetch

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install langchain langchain-community langchainhub langchain-openai  langchain-huggingface
#

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install langchain-experimental

from langchain.agents import AgentExecutor, load_tools
from langchain.agents.format_scratchpad import format_log_to_str
from langchain.agents.output_parsers import (
    ReActJsonSingleInputOutputParser,
)
from langchain.agents import AgentExecutor, create_tool_calling_agent,create_react_agent

from langchain.tools.render import render_text_description
from langchain import hub
from langchain.tools import tool

from huggingface_hub import HfFolder, notebook_login

from typing import List, Dict, Any, Optional, Type
from langchain.prompts import PromptTemplate
from langchain.tools import BaseTool
from pydantic import BaseModel, Field

from bs4 import BeautifulSoup
import requests
import json

"""# Using Llama3 Model via Ollama"""

OLLAMA_MODEL='llama3'

# Set it at the OS level
import os
os.environ['OLLAMA_MODEL'] = OLLAMA_MODEL
!echo $OLLAMA_MODEL

import subprocess
import time

# Start ollama as a backrgound process
command = "nohup ollama serve&"

# Use subprocess.Popen to start the process in the background
process = subprocess.Popen(command,
                            shell=True,
                           stdout=subprocess.PIPE,
                           stderr=subprocess.PIPE)
print("Process ID:", process.pid)
time.sleep(5)  # Makes Python wait for 5 seconds

!ollama -v

!ollama pull llama3

from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    api_key="ollama",
    model=OLLAMA_MODEL,
    base_url="http://localhost:11434/v1",
    temperature= 0,
)

#Change the host with base_url of your own hosting

# from langchain_huggingface import HuggingFacePipeline

# llm = HuggingFacePipeline.from_model_id(
#     model_id="mistralai/Mistral-7B-Instruct-v0.2",
#     task="text-generation",
#     pipeline_kwargs={
#         "max_new_tokens": 100,
#         "top_k": 50,
#         "temperature": 0.1,
#     },
# )

"""# Creating Custom function as Tool

The following block should be replaced with ProductAPI from Amazon, remember to insert your own affialite key
"""

import ast

def fetch_product_urls_keywords_per_domain(Keywords):


    domains = ["in", "com", "co.uk", "de", "fr", "it", "ae"]
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
        'Accept-Language': 'en-US, en;q=0.5'
    }
    max_products = 3

    all_valid_product_urls = []

    for domain in domains:
        try:
            lang_offer = get_language_offer_keyword(domain)
            search_url = f"https://www.amazon.{domain}/s?k={lang_offer}+{'+'.join(Keywords)}"


            webpage = requests.get(search_url, headers=headers)
            webpage.raise_for_status()  # Raise an error for bad requests
            soup = BeautifulSoup(webpage.content, "html.parser")

            product_links = soup.find_all("a", attrs={'class': 'a-link-normal s-no-outline'})
            product_urls = [f"https://www.amazon.{domain}{link.get('href')}" for link in product_links]

            valid_product_urls = []
            for url in product_urls:
                parts = url.split('/ref=sr', 1)
                if len(parts) > 1:
                    valid_product_urls.append(parts[0])

            all_valid_product_urls.extend(valid_product_urls[:max_products])

        except requests.RequestException as e:
            print(f"Error fetching product URLs from {search_url}: {e}")

    return all_valid_product_urls

"""The previous block will be replaced with one containing more direct code for the implementation of the affiliate program in the next version"""

def fetch_product_details(url):
    headers= ({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
        'Accept-Language': 'en-US, en;q=0.5'
      })
    try:
        webpage = requests.get(url, headers=headers)
        soup = BeautifulSoup(webpage.content, "html.parser")

        product_details = {
            "url": url,  # Include the URL in the product details
            "title": get_title(soup),
            "price": (get_price(soup) ,get_currency(url)),
            "currency": get_currency(url),
            "rating": get_rating(soup),
            "reviews": get_review_count(soup),
            "availability": get_availability(soup),
            "description": get_description(soup)
        }

        return product_details
    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None


def scrape_amazon_products_visit_url(urls):

    product_data = {"products": []}

    for url in urls:
        product_details = fetch_product_details(url)
        if product_details["availability"] != "Not Available":
            product_data["products"].append(product_details)

    save_to_json(product_data, "amazon_product_fetch.json")


    return product_data

# Function to extract Product Title
def get_title(soup):
    try:
        title = soup.find("span", attrs={"id": 'productTitle'}).text.strip()
    except AttributeError:
        title = ""
    return title

# Function to extract Product Price
def get_price(soup):
    price = ""
    try:
        price = soup.find("span", attrs={'id': 'priceblock_ourprice'}).text.strip()
    except AttributeError:
        try:
            price = soup.find("span", attrs={'id': 'priceblock_dealprice'}).text.strip()
        except AttributeError:
            try:
                price = soup.find("span", attrs={'class': 'a-price-whole'}).text.strip()
                price_fraction = soup.find("span", attrs={'class': 'a-price-fraction'}).text.strip()
                price = price + price_fraction
            except AttributeError:
                price = ""
    return price

# Function to extract Product Rating
def get_rating(soup):
    try:
        rating = soup.find("i", attrs={'class': 'a-icon a-icon-star a-star-4-5'}).text.strip()
    except AttributeError:
        try:
            rating = soup.find("span", attrs={'class': 'a-icon-alt'}).text.strip()
        except AttributeError:
            rating = ""
    return rating

# Function to extract Number of User Reviews
def get_review_count(soup):
    try:
        review_count = soup.find("span", attrs={'id': 'acrCustomerReviewText'}).text.strip()
    except AttributeError:
        review_count = ""
    return review_count

# Function to extract Availability Status
def get_availability(soup):
    try:
        availability = soup.find("div", attrs={'id': 'availability'}).find("span").text.strip()
    except AttributeError:
        availability = "Not Available"
    return availability

# Function to extract Product Description
def get_description(soup):
    try:
        description = soup.find("div", attrs={'id': 'productDescription'}).find("p").text.strip()
    except AttributeError:
        try:
            description = soup.find("div", attrs={'id': 'feature-bullets'}).find_all("span", class_="a-list-item")
            description = ' '.join([desc.text.strip() for desc in description])
        except AttributeError:
            description = ""
    return description

# Function to determine currency based on URL
def get_currency(url):
    if ".co.uk" in url:
        return "GBP"
    elif ".ae" in url:
        return "AED"
    elif ".it" in url:
        return "EUR"
    elif ".de" in url:
        return "EUR"
    elif ".fr" in url:
        return "EUR"
    elif ".com" in url:
        return "USD"
    return "USD"  # Default currency



def get_language_offer_keyword(url):
    language_keywords = {
        "co.uk": "offerte",
        "ae": "offer",
        "it": "offerta",
        "de": "Angebot",
        "fr": "offerte",
        "com": "offerte",
        "in": "offer"
    }
    # Access the language keyword using the domain as the key
    return language_keywords.get(url, "offer")

# Function to save product details to a JSON file
def save_to_json(data, filename):
    with open(filename, 'w') as json_file:
        json.dump(data, json_file, indent=4)

# Define input model for fetching product URLs
class FetchProductURLsInput(BaseModel):
    """Input model for fetching product URLs."""
    keywords: List[str] = Field(description="List of keywords to search for fetching Relevent URL Products.")

# Define the tool for fetching product URLs
class FetchProductURLsTool(BaseTool):
    name = "fetch_product_urls_tool"
    description = "Fetches product URLs based on keywords"

    def _run(self, keywords: List[str]) -> Dict[str, List[str]]:
        # Call the fetch_product_urls_keywords_per_domain function with the list of keywords
        response = fetch_product_urls_keywords_per_domain(keywords)
        return response

    def _irun(self, keywords: List[str]):
        raise NotImplementedError("This tool does not support async")

    args_schema: Optional[Type[BaseModel]] = FetchProductURLsInput

# Define input model for fetching product details
class FetchDetailsProductsInput(BaseModel):
    """Input model for fetch Products Details based on Keywords as List"""
    URLs: List[str] = Field(description="List keywords to Fetch Relevent Product Details")

class FetchDetailsProductsTool(BaseTool):
    name = "extraction_details_products_urls_tool"
    description = "Fetch Products Details for uswr based on Keywords and only take Keywords"

    def _run(self, keywords: List[str]):
        Links = fetch_product_urls_keywords_per_domain(keywords)
        response = scrape_amazon_products_visit_url(Links)
        return {"Products": response ,"Link":Links}

    def _irun(self, keywords: List[str]):
        raise NotImplementedError("This tool does not support async")

    args_schema: Optional[Type[BaseModel]] = FetchProductURLsInput

@tool("fetch_product_marketplace_from_keywords_tool",return_direct = True)
def fetch_product_marketplace_from_keywords(Keywords)-> Dict[str, Any]:
    """
    Fetch Products Details for uswr based on Keywords and only take Keywords
    """
    URLs = fetch_product_urls_keywords_per_domain(Keywords)
    Details = scrape_amazon_products_visit_url(URLs)
    args_schema: Optional[Type[BaseModel]] = FetchDetailsProductsInput


    return {"Products":Details , "Links":URLs}

tools = [fetch_product_marketplace_from_keywords]

llm = llm.bind_tools(tools)

"""# Creating ReAct Agent using LLM and Tools

**ReAct Paper Link** - https://arxiv.org/pdf/2210.03629.pdf

![image.png](attachment:175725eb-6a84-4215-b463-e55aaf1c09c7.png)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# 
# Keywords = """
# {Keywords}
# """
# 
# # setup ReAct style prompt
# prompt = hub.pull("hwchase17/react-json")
# prompt = prompt.partial(
#     tools=render_text_description(tools),
#     tool_names=", ".join([t.name for t in tools]),
#     Keywords = Keywords
# )
# 
# chat_model_with_stop = llm.bind(stop=["\nFinal Answer"])
# 
# agent = (
#     {
#         "input": lambda x: x["input"],
#         "Keywords":lambda x: x["Keywords"],
#         "agent_scratchpad": lambda x: format_log_to_str(x["intermediate_steps"]),
#     }
#     | prompt
#     | chat_model_with_stop
#     | ReActJsonSingleInputOutputParser()
# )
# 
# Handling = "only keywords List"
# 
# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=Handling,return_intermediate_steps=True,max_iterations=8)

prompt

AgentParser = ChatOpenAI(
    api_key="ollama",
    model=OLLAMA_MODEL,
    base_url="http://localhost:11434/v1",
    temperature= 0,
)

#Change the host with base_url of your own hosting

SerializePrompt = PromptTemplate(template="""**Objective**: Configure an affiliate program API to retrieve products that match specific user needs and market trends from an e-commerce platform.
**Data Elements**:
1. **API Parameter Set**: Utilize API_set_affiliates` to access a comprehensive list of adjustable parameters such as price range, product categories, brand preferences, and user ratings.
2. **Keywords**: Set  extracted Keywords to refine the search based on trends, user behaviors, or specific queries.
3. **User Prompt**: Analyze `{user_prompt}` for additional context, capturing specific user needs or product preferences.
**Steps**:
- Identify key themes from the keywords and user prompt.
- Determine the most relevant API parameters to enhance search relevance and commercial potential.
**Parameters to Configure**:
- **Actor**
- **Artist**
- **Author**
- **Availability**: ["Available", "IncludeOutOfStock"]
- **Brand**
- **BrowseNodeId**
- **Condition**: ["New", "Used", "Collectible", "Refurbished"]
- **CurrencyOfPreference**: (ISO 4217 codes, e.g., "USD", "EUR")
- **DeliveryFlags**: ["Prime", "FreeShipping", "AmazonGlobal"]
- **ItemCount**: (1-10)
- **ItemPage**: (1-10)
- **Keywords**
# Stampa il risultato
print("The result is:", result
- **LanguagesOfPreference**: (ISO language-region codes, e.g., "en-US", "fr-FR")
- **Marketplace**: (e.g., "www.amazon.com")
- **MaxPrice**
- **Merchant**: ["All", "Amazon"]
- **MinPrice**
- **MinReviewsRating**: (1-5)
- **MinSavingPercent**: (1-100)
- **OfferCount**
- **PartnerTag**
- **PartnerType**: "Associates"
- **Properties**
- **Resources**: ["Images.Primary.Medium", "ItemInfo.Title", "Offers.Listings.Price"]
- **SearchIndex**
- **SortBy**: ["Price:HighToLow", "Price:LowToHigh", "Relevance", "NewestArrivals"]
- **Title**
**Expected Output**:
- Configure the above parameters within a JSON format to execute the API request. The API should return a dataset of products closely aligned with the user input and preferences, optimizing engagement and conversion rates.
Give me only the JSON as output, no other text than that

Finle Stage is to pass JSON file into tool AmazonAPI

""",

  input_variables=['user_prompt'],

                        )

from langchain_core.output_parsers import JsonOutputParser


user_prompt = "Tell me which is the best smartphone in the market under 1000$"

Serialize_Chain = SerializePrompt | AgentParser | JsonOutputParser()

Amazon_Json = Serialize_Chain.invoke({"user_prompt": user_prompt})

Amazon_Json

Keywords=Amazon_Json['api_set_affiliates']['Keywords']

"""# Running the Agent with our input query
As we are using small OS model, some times instead of giving final value it gives placeholders. So just run the below cell again if you face this issue.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import time
# 
# start_time = time.time()
# 
# def get_amazon_api_set_affiliates(user_prompt):
#     Amazon_Json = Serialize_Chain.invoke({"user_prompt": user_prompt})
#     Keywords = Amazon_Json['api_set_affiliates']['Keywords']
#     print(Amazon_Json)
# 
#     return Keywords , Amazon_Json
# 
# def run_agent(user_prompt, keywords):
#     respond = agent_executor.invoke(
#         {
#             "input": user_prompt,
#             "Keywords": keywords,
#         }
#     )
#     return respond['output']
# 
#     #This is where you should replace the prompt calling the one in the integration file
# user_prompt = "i need to clean my floor"
# 
# keywords , Amazon_Json = get_amazon_api_set_affiliates(user_prompt)
# 
# 
#

Amazon_Json

output = run_agent(user_prompt, keywords)

print("Output:", output)

output

keywords